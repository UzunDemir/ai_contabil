# ... (–ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–¥ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–æ –∫–ª–∞—Å—Å–∞ KnowledgeBase)

class KnowledgeBase:
    # ... (–ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∞ –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

    def add_document(self, file_content, file_name):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        if file_name in self.uploaded_files:
            st.warning(f"–î–æ–∫—É–º–µ–Ω—Ç '{file_name}' —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return False
        
        success = self.load_pdf(file_content, file_name)
        if success:
            self.update_cache()
        return success
    
    def update_cache(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        cache_file = os.path.join(CACHE_DIR, "knowledge_base.cache")
        with open(cache_file, 'wb') as f:
            pickle.dump({
                'files': self.uploaded_files,
                'chunks': self.chunks,
                'doc_texts': self.doc_texts
            }, f)
    
    def clear(self):
        """–û—á–∏—â–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        self.chunks = []
        self.uploaded_files = []
        self.doc_texts = []
        self.tfidf_matrix = None
        cache_file = os.path.join(CACHE_DIR, "knowledge_base.cache")
        if os.path.exists(cache_file):
            os.remove(cache_file)

# ... (–ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–¥ –¥–æ —Ä–∞–∑–¥–µ–ª–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)

# –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
with st.sidebar.expander("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"):
    uploaded_files = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ PDF-–¥–æ–∫—É–º–µ–Ω—Ç—ã", 
        type="pdf", 
        accept_multiple_files=True,
        help="–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF-—Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
    )
    
    if uploaded_files and st.button("–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã"):
        for uploaded_file in uploaded_files:
            with st.spinner(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {uploaded_file.name}..."):
                if st.session_state.knowledge_base.add_document(
                    uploaded_file.read(), 
                    uploaded_file.name
                ):
                    st.success(f"–î–æ–∫—É–º–µ–Ω—Ç '{uploaded_file.name}' —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω")
                else:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ '{uploaded_file.name}'")
        st.rerun()

# –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
with st.sidebar.expander("‚öôÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π"):
    if st.button("–û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"):
        st.session_state.knowledge_base.clear()
        st.success("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ—á–∏—â–µ–Ω–∞")
        st.rerun()
    
    st.download_button(
        label="–≠–∫—Å–ø–æ—Ä—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π",
        data=pickle.dumps(st.session_state.knowledge_base),
        file_name="knowledge_base.pkl",
        mime="application/octet-stream",
        help="–°–∫–∞—á–∞—Ç—å —Ç–µ–∫—É—â—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
    )

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á–∞—Ç–∞
st.title("üìö AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Å—Ç–∞–º")
st.caption("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω–∏—è
if st.session_state.knowledge_base.uploaded_files:
    with st.expander("üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã", expanded=True):
        cols = st.columns([4, 1])
        cols[0].subheader("–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        cols[1].subheader("–î–µ–π—Å—Ç–≤–∏—è")
        
        for doc in sorted(st.session_state.knowledge_base.uploaded_files):
            col1, col2 = st.columns([4, 1])
            col1.markdown(f"- {doc}")
            if col2.button("üóëÔ∏è", key=f"del_{doc}", help="–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"):
                # –£–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
                st.session_state.knowledge_base.chunks = [
                    chunk for chunk in st.session_state.knowledge_base.chunks 
                    if chunk.doc_name != doc
                ]
                st.session_state.knowledge_base.uploaded_files.remove(doc)
                st.session_state.knowledge_base.doc_texts = [
                    text for i, text in enumerate(st.session_state.knowledge_base.doc_texts)
                    if st.session_state.knowledge_base.chunks[i].doc_name != doc
                ]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
                if st.session_state.knowledge_base.doc_texts:
                    st.session_state.knowledge_base.tfidf_matrix = (
                        st.session_state.knowledge_base.vectorizer.fit_transform(
                            st.session_state.knowledge_base.doc_texts
                        )
                    )
                else:
                    st.session_state.knowledge_base.tfidf_matrix = None
                
                st.session_state.knowledge_base.update_cache()
                st.success(f"–î–æ–∫—É–º–µ–Ω—Ç '{doc}' —É–¥–∞–ª–µ–Ω")
                st.rerun()

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
def display_relevant_chunks(relevant_chunks):
    if not relevant_chunks:
        return
    
    with st.expander("üîç –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
        for i, (text, doc_name, page_num) in enumerate(relevant_chunks, 1):
            st.markdown(f"**–§—Ä–∞–≥–º–µ–Ω—Ç #{i}** (–∏–∑ {doc_name}, —Å—Ç—Ä. {page_num}):")
            st.text(text[:500] + "..." if len(text) > 500 else text)
            st.divider()

# –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    relevant_chunks = st.session_state.knowledge_base.find_most_relevant_chunks(prompt)
    
    if not relevant_chunks:
        response_text = "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö ‚ùå"
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        with st.chat_message("assistant"):
            st.markdown(response_text)
    else:
        context = "\n\n".join([f"–î–æ–∫—É–º–µ–Ω—Ç: {doc_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}\n{text}" 
                             for text, doc_name, page_num in relevant_chunks])
        
        full_prompt = f"""–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π —Å–æ–≤–µ—Ç–Ω–∏–∫.
        –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤ (–≤–Ω–∏–∑—É —É–∫–∞–∑–∞–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∏):

–í–æ–ø—Ä–æ—Å: {prompt}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
1. –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥
2. –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
3. –¢–æ—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": full_prompt}],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–æ–∫—É–º–µ–Ω—Ç—ã..."):
            start_time = datetime.now()
            
            try:
                response = requests.post(url, headers=headers, json=data, timeout=30)
                
                if response.status_code == 200:
                    response_data = response.json()
                    full_response = response_data['choices'][0]['message']['content']
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                    sources = "\n\nüìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n" + "\n".join(
                        [f"- {doc_name}, —Å—Ç—Ä. {page_num}" for _, doc_name, page_num in relevant_chunks]
                    )
                    full_response += sources
                    
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response)
                        display_relevant_chunks(relevant_chunks)
                    
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    st.toast(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {duration:.2f} —Å–µ–∫", icon="‚è±Ô∏è")
                else:
                    st.error(f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}")
            except requests.exceptions.Timeout:
                st.error("–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç API")
            except Exception as e:
                st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")
                logging.error(f"API request error: {str(e)}")

# ... (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
