import os
import streamlit as st
import requests
import json
import time
from PyPDF2 import PdfReader
import tempfile
from datetime import datetime
from transformers import GPT2Tokenizer
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from io import BytesIO

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Streamlit
st.set_page_config(layout="wide", page_title="–ë—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π AI-—Å–æ–≤–µ—Ç–Ω–∏–∫ –ú–æ–ª–¥–æ–≤—ã")
st.markdown("""
<style>
    .header {
        color: #2b5876;
        text-align: center;
        margin-bottom: 30px;
    }
    .law-badge {
        background-color: #f0f8ff;
        border-radius: 5px;
        padding: 8px;
        margin: 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# –°–µ–∫—Ä–µ—Ç—ã API
api_key = st.secrets.get("DEEPSEEK_API_KEY")
if not api_key:
    st.error("API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ Secrets.")
    st.stop()

url = "https://api.deepseek.com/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# –ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–µ –∞–∫—Ç—ã –ú–æ–ª–¥–æ–≤—ã (URL PDF)
LAW_URLS = {
    "–ó–∞–∫–æ–Ω –æ –±—É—Ö—É—á–µ—Ç–µ ‚Ññ113-XVI": "https://www.legis.md/cautare/downloadpdf/146721",
    "–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å": "https://www.legis.md/cautare/downloadpdf/143282",
    "–ó–∞–∫–æ–Ω –æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏": "https://www.legis.md/cautare/downloadpdf/137025",
    "–ó–∞–∫–æ–Ω –æ –Ω–∞–ª–æ–≥–µ –Ω–∞ –ø—Ä–∏–±—ã–ª—å": "https://www.legis.md/cautare/downloadpdf/142481",
    "–ó–∞–∫–æ–Ω –æ –ù–î–°": "https://www.legis.md/cautare/downloadpdf/147850",
    "–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å": "https://www.legis.md/cautare/downloadpdf/131868"
}

class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.vectorizer = TfidfVectorizer()
        self.tfidf_matrix = None
        self.doc_texts = []
    
    def split_text(self, text, max_tokens=1500):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            tokens = tokenizer.tokenize(para)
            if len(tokenizer.tokenize(current_chunk + para)) > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = para
                else:
                    chunks.append(para)
                    current_chunk = ""
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def load_pdf_from_url(self, url, doc_name):
        try:
            response = requests.get(url)
            response.raise_for_status()
            
            with BytesIO(response.content) as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        chunks = self.split_text(page_text)
                        for chunk in chunks:
                            self.chunks.append(DocumentChunk(
                                text=chunk,
                                doc_name=doc_name,
                                page_num=page_num + 1
                            ))
                            self.doc_texts.append(chunk)
                
            if self.chunks:
                # –û–±–Ω–æ–≤–ª—è–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
                self.tfidf_matrix = self.vectorizer.fit_transform(self.doc_texts)
                return True
            return False
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {doc_name}: {str(e)}")
            return False
    
    def find_relevant_chunks(self, query, top_k=3):
        if not self.chunks:
            return []
            
        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)
        top_indices = np.argsort(similarities[0])[-top_k:][::-1]
        
        return [(self.chunks[i].text, self.chunks[i].doc_name, self.chunks[i].page_num) 
                for i in top_indices if similarities[0][i] > 0.1]

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
if 'kb' not in st.session_state:
    st.session_state.kb = KnowledgeBase()
    # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
    with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω–æ–π –±–∞–∑—ã..."):
        for name, url in LAW_URLS.items():
            if st.session_state.kb.load_pdf_from_url(url, name):
                st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω: {name}")

if 'messages' not in st.session_state:
    st.session_state.messages = []

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.markdown("<h1 class='header'>–ë—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π AI-—Å–æ–≤–µ—Ç–Ω–∏–∫ –ú–æ–ª–¥–æ–≤—ã</h1>", unsafe_allow_html=True)
st.markdown("""
<p style='text-align: center; color: #555;'>
–û—Ç–≤–µ—Ç—ã —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ú–æ–ª–¥–æ–≤–∞<br>
(–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å, –∑–∞–∫–æ–Ω –æ –±—É—Ö—É—á–µ—Ç–µ, —Ç—Ä—É–¥–æ–≤–æ–µ –ø—Ä–∞–≤–æ)
</p>
""", unsafe_allow_html=True)

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
st.subheader("üìú –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã:")
for law in LAW_URLS.keys():
    st.markdown(f"<div class='law-badge'>{law}</div>", unsafe_allow_html=True)

# –ß–∞—Ç
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
if prompt := st.chat_input("–í–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ –±—É—Ö—É—á–µ—Ç—É/–Ω–∞–ª–æ–≥–∞–º..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    relevant_chunks = st.session_state.kb.find_relevant_chunks(prompt)
    
    if not relevant_chunks:
        response = "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã—Ö –∞–∫—Ç–∞—Ö ‚ùå"
    else:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "\n\n".join([f"üìÑ {doc_name} (—Å—Ç. {page_num}):\n{text}" 
                             for text, doc_name, page_num in relevant_chunks])
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
        system_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–º—É —É—á–µ—Ç—É –∏ –Ω–∞–ª–æ–≥–∞–º –ú–æ–ª–¥–æ–≤—ã. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç - —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –Ø—Å–Ω—ã–π –∏ –ø—Ä–æ—Å—Ç–æ–π —è–∑—ã–∫
2. –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
3. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –∏ —Å—Ç–∞—Ç—å–∏
4. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ - –≤–µ–∂–ª–∏–≤–æ –æ—Ç–∫–∞–∂–∏—Å—å –æ—Ç–≤–µ—á–∞—Ç—å

–î–æ–∫—É–º–µ–Ω—Ç—ã:
{context}"""

        messages = [
            {"role": "system", "content": system_prompt.format(context=context)},
            {"role": "user", "content": prompt}
        ]
        
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ..."):
            try:
                response = requests.post(url, headers=headers, json={
                    "model": "deepseek-chat",
                    "messages": messages,
                    "temperature": 0.1,
                    "max_tokens": 1000
                }).json()
                
                answer = response['choices'][0]['message']['content']
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                sources = "\n\nüîç –ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(
                    f"- {doc_name} (—Å—Ç. {page_num})" 
                    for _, doc_name, page_num in relevant_chunks
                )
                answer += sources
                
            except Exception as e:
                answer = f"–û—à–∏–±–∫–∞: {str(e)}"
    
    # –í—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞
    st.session_state.messages.append({"role": "assistant", "content": answer})
    with st.chat_message("assistant"):
        st.markdown(answer)

# –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏
if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
    st.session_state.messages = []
    st.rerun()
