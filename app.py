import os
import io
import json
import requests
import streamlit as st
import PyPDF2

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# üîê –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤–∞—à API-–∫–ª—é—á –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
API_KEY = os.environ.get("DEEPSEEK_API_KEY")

# üîó –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
# LEGIS_URLS = [
#     "https://www.legis.md/cautare/downloadpdf/146721",
#     "https://www.legis.md/cautare/downloadpdf/143282",
#     "https://www.legis.md/cautare/downloadpdf/137025",
#     "https://www.legis.md/cautare/downloadpdf/142481",
#     "https://www.legis.md/cautare/downloadpdf/147850",
#     "https://www.legis.md/cautare/downloadpdf/131868"
# ]

LEGIS_URLS = [
    "https://www.legis.md/cautare/downloadpdf/146721",
    "https://www.legis.md/cautare/downloadpdf/143282",
    "https://www.legis.md/cautare/downloadpdf/137025",
    "https://www.legis.md/cautare/downloadpdf/142481",
    "https://www.legis.md/cautare/downloadpdf/147850",
    "https://www.legis.md/cautare/downloadpdf/131868"
]

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

for url in urls:
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        filename = url.split("/")[-1] + ".pdf"
        with open(filename, "wb") as f:
            f.write(response.content)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {filename}")
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url} ‚Äî {e}")



class KnowledgeBase:
    def __init__(self):
        self.text_chunks = []
        self.chunk_sources = []
        self.vectorizer = TfidfVectorizer()
        self.tfidf_matrix = None
        self.uploaded_files = []

    def extract_text_from_pdf(self, file_content):
        text_pages = []
        reader = PyPDF2.PdfReader(io.BytesIO(file_content))
        for i, page in enumerate(reader.pages):
            try:
                text = page.extract_text()
                if text:
                    text_pages.append((text.strip(), i + 1))
            except Exception:
                continue
        return text_pages

    def split_into_chunks(self, text, max_tokens=500):
        sentences = text.split(". ")
        chunks = []
        current = []
        length = 0
        for sent in sentences:
            l = len(sent.split())
            if length + l > max_tokens:
                if current:
                    chunks.append(". ".join(current))
                    current = []
                    length = 0
            current.append(sent)
            length += l
        if current:
            chunks.append(". ".join(current))
        return chunks

    def load_pdf(self, file_content, file_name):
        pages = self.extract_text_from_pdf(file_content)
        for text, page_num in pages:
            chunks = self.split_into_chunks(text)
            for chunk in chunks:
                self.text_chunks.append(chunk)
                self.chunk_sources.append((file_name, page_num))
        self.uploaded_files.append(file_name)
        self._fit_vectorizer()

    def _fit_vectorizer(self):
        if self.text_chunks:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.text_chunks)

    def find_most_relevant_chunks(self, query, top_k=3):
        if not self.tfidf_matrix:
            return []
        query_vec = self.vectorizer.transform([query])
        scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]
        top_indices = scores.argsort()[::-1][:top_k]
        return [(self.text_chunks[i], *self.chunk_sources[i]) for i in top_indices]


@st.cache_resource
def get_knowledge_base():
    return KnowledgeBase()


st.set_page_config(page_title="–°–æ–≤–µ—Ç–Ω–∏–∫ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∞", page_icon="üìö")
st.title("üìö –°–æ–≤–µ—Ç–Ω–∏–∫ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∞")
st.caption("–û—Ç–≤–µ—á–∞–µ—Ç —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ú–æ–ª–¥–æ–≤–∞")

if "knowledge_base" not in st.session_state:
    st.session_state.knowledge_base = get_knowledge_base()

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
st.subheader("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
for url in LEGIS_URLS:
    try:
        response = requests.get(url)
        if response.status_code == 200:
            filename = url.split("/")[-1] + ".pdf"
            with st.spinner(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞: {filename}"):
                kb = st.session_state.knowledge_base
                kb.load_pdf(response.content, filename)
                st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {filename}")
        else:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url}")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
st.divider()
st.subheader("üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
if st.session_state.knowledge_base.uploaded_files:
    st.write("\n".join(f"- {name}" for name in st.session_state.knowledge_base.uploaded_files))
else:
    st.write("–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

# –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞
st.divider()
st.subheader("‚ùì –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å")

user_input = st.text_input("–í–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏–∏:")
if user_input:
    kb = st.session_state.knowledge_base
    references = kb.find_most_relevant_chunks(user_input, top_k=3)

    if not references:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.")
    else:
        context = "\n\n".join([f"(–ò—Å—Ç–æ—á–Ω–∏–∫: {doc}, —Å—Ç—Ä. {page})\n{chunk}" for chunk, doc, page in references])

        prompt = f"""–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π —Å–æ–≤–µ—Ç–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤ (–≤–Ω–∏–∑—É —É–∫–∞–∑–∞–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∏):

–§–†–ê–ì–ú–ï–ù–¢–´ –î–û–ö–£–ú–ï–ù–¢–û–í:
{context}

–í–û–ü–†–û–°:
{user_input}

–û–¢–í–ï–¢:"""

        body = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.5,
            "top_p": 0.9,
            "max_tokens": 1024,
            "frequency_penalty": 0,
            "presence_penalty": 0
        }

        url = "https://api.deepseek.com/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}"
        }

        with st.spinner("–§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç..."):
            response = requests.post(url, headers=headers, data=json.dumps(body))
            if response.status_code == 200:
                reply = response.json()["choices"][0]["message"]["content"]
                st.success("–û—Ç–≤–µ—Ç:")
                st.markdown(reply)
                st.markdown("---")
                for chunk, doc, page in references:
                    st.markdown(f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** {doc}, —Å—Ç—Ä. {page}")
            else:
                st.error(f"–û—à–∏–±–∫–∞ API: {response.text}")
