import os
import pickle
import hashlib
import streamlit as st
import requests
import numpy as np
from PyPDF2 import PdfReader
from datetime import datetime
from transformers import GPT2Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CACHE_DIR = "cache"
DOCS_DIR = "docs"
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(DOCS_DIR, exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Streamlit
st.set_page_config(layout="wide", initial_sidebar_state="auto")
st.markdown("""
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    .css-1jc7ptx, .e1ewe7hr3, .viewerBadge_container__1QSob, 
    .styles_viewerBadge__1yB5_, .viewerBadge_link__1S137, 
    .viewerBadge_text__1JaDK, #MainMenu, footer, header { 
        display: none !important; 
    }
    .center {
        display: flex;
        justify-content: center;
        align-items: center;
        text-align: center;
        flex-direction: column;
        margin-top: 0vh;
    }
</style>
""", unsafe_allow_html=True)

st.sidebar.title("–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞")
st.sidebar.title("TEST-passer (AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Å—Ç–∞–º)")

#######
# –î–æ–±–∞–≤—å—Ç–µ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ (–ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤)
import logging
logging.basicConfig(level=logging.INFO)


st.sidebar.divider()
st.sidebar.write(
    """
    –≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ –ø–∞–ø–∫–∏ docs –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
    –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É—é—Ç—Å—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã.
    """
)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∏–ª—å –¥–ª—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤
st.markdown("""
    <style>
    .center {
        display: flex;
        justify-content: center;
        align-items: center;
        text-align: center;
        flex-direction: column;
        margin-top: 0vh;
    }
    </style>
    <div class="center">
        <img src="https://github.com/UzunDemir/mnist_777/blob/main/200w.gif?raw=true">
        <h1>TEST-passer</h1>
        <h2>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Å—Ç–∞–º</h2>
        <p>(—Å—Ç—Ä–æ–≥–æ –ø–æ —É—á–µ–±–Ω—ã–º –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º)</p>
    </div>
    """, unsafe_allow_html=True)

st.divider()

# –ü–æ–ª—É—á–µ–Ω–∏–µ API –∫–ª—é—á–∞
api_key = st.secrets.get("DEEPSEEK_API_KEY")
if not api_key:
    st.error("API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ Secrets.")
    st.stop()

url = "https://api.deepseek.com/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.vectorizer = TfidfVectorizer(stop_words='english')
        self.tfidf_matrix = None
        self.doc_texts = []
        self.loaded_files = set()

# –ì–¥–µ-—Ç–æ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ —Å–∞–π–¥–±–∞—Ä–µ)
with st.sidebar:
    if st.button("üõ† –¢–µ—Å—Ç –∫—ç—à–∞ (DEBUG)"):
        try:
            st.info("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è...")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∞
            test_kb = KnowledgeBase()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É
            st.write("1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–ø–æ–∫:")
            st.code(f"DOCS_DIR: {os.listdir(DOCS_DIR)}\nCACHE_DIR: {os.listdir(CACHE_DIR)}")
            
            # –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF
            st.write("2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
            test_kb.load_with_cache()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.write("3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
            if test_kb.chunks:
                st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(test_kb.chunks)} —á–∞–Ω–∫–æ–≤")
                st.code(f"–ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫:\n{test_kb.chunks[-1].text[:200]}...")
            else:
                st.error("‚ùå –ß–∞–Ω–∫–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã!")
                
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –∫—ç—à–∞
            st.write("4. –°–æ–¥–µ—Ä–∂–∏–º–æ–µ cache/:")
            cache_files = os.listdir(CACHE_DIR)
            if cache_files:
                st.success(f"–ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –∫—ç—à–∞: {cache_files}")
                if "knowledge_base.cache" in cache_files:
                    st.code(f"–†–∞–∑–º–µ—Ä –∫—ç—à–∞: {os.path.getsize(os.path.join(CACHE_DIR, 'knowledge_base.cache'))} –±–∞–π—Ç")
            else:
                st.error("–§–∞–π–ª—ã –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
                
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {str(e)}")
            logging.exception("–û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∫—ç—à–∞:")
            #################


    
    def split_text(self, text, max_tokens=2000):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            tokens = tokenizer.tokenize(para)
            if len(tokenizer.tokenize(current_chunk + para)) > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = para
                else:
                    chunks.append(para)
                    current_chunk = ""
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def process_pdf(self, file_path, file_name):
        try:
            with open(file_path, 'rb') as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        chunks = self.split_text(page_text)
                        for chunk in chunks:
                            self.chunks.append(DocumentChunk(
                                text=chunk,
                                doc_name=file_name,
                                page_num=page_num + 1
                            ))
                            self.doc_texts.append(chunk)
                return True
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF {file_name}: {e}")
            return False
    
    def build_vectorizer(self):
        if self.doc_texts:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.doc_texts)
    
    def find_most_relevant_chunks(self, query, top_k=3):
        if not self.chunks:
            return []
            
        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)
        top_indices = np.argsort(similarities[0])[-top_k:][::-1]
        
        return [(self.chunks[i].text, self.chunks[i].doc_name, self.chunks[i].page_num) 
                for i in top_indices if similarities[0][i] > 0.1]
    
    def save_to_cache(self):
        cache_file = os.path.join(CACHE_DIR, "knowledge_base.cache")
        with open(cache_file, 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'doc_texts': self.doc_texts,
                'vectorizer': self.vectorizer,
                'tfidf_matrix': self.tfidf_matrix,
                'loaded_files': self.loaded_files
            }, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–æ–≤
        hash_file = os.path.join(CACHE_DIR, "files_hash.txt")
        with open(hash_file, 'w') as f:
            f.write(self.get_files_hash())
    
    def load_from_cache(self):
        cache_file = os.path.join(CACHE_DIR, "knowledge_base.cache")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    self.chunks = data['chunks']
                    self.doc_texts = data['doc_texts']
                    self.vectorizer = data['vectorizer']
                    self.tfidf_matrix = data['tfidf_matrix']
                    self.loaded_files = data['loaded_files']
                return True
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
                return False
        return False
    
    def get_files_hash(self):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à –≤—Å–µ—Ö PDF-—Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        hash_obj = hashlib.sha256()
        for filename in sorted(os.listdir(DOCS_DIR)):
            if filename.lower().endswith('.pdf'):
                filepath = os.path.join(DOCS_DIR, filename)
                with open(filepath, 'rb') as f:
                    while chunk := f.read(8192):
                        hash_obj.update(chunk)
        return hash_obj.hexdigest()
    
    def load_with_cache(self):
        """–£–º–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ö–µ—à–∞ —Ñ–∞–π–ª–æ–≤"""
        cache_file = os.path.join(CACHE_DIR, "knowledge_base.cache")
        hash_file = os.path.join(CACHE_DIR, "files_hash.txt")
        
        current_hash = self.get_files_hash()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ö–µ—à –∏ –æ–Ω —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ç–µ–∫—É—â–∏–º - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫–µ—à–∞
        if os.path.exists(hash_file) and os.path.exists(cache_file):
            with open(hash_file, 'r') as f:
                saved_hash = f.read().strip()
            
            if saved_hash == current_hash:
                if self.load_from_cache():
                    st.success("–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞")
                    return True
        
        # –ï—Å–ª–∏ –∫–µ—à –Ω–µ–≤–∞–ª–∏–¥–µ–Ω - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º
        st.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        self.chunks = []
        self.doc_texts = []
        self.loaded_files = set()
        
        for filename in os.listdir(DOCS_DIR):
            if filename.lower().endswith('.pdf'):
                file_path = os.path.join(DOCS_DIR, filename)
                if self.process_pdf(file_path, filename):
                    self.loaded_files.add(filename)
                    st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {filename}")
        
        self.build_vectorizer()
        self.save_to_cache()
        st.success("–ö—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω!")
        return True

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
if 'knowledge_base' not in st.session_state:
    st.session_state.knowledge_base = KnowledgeBase()
    st.session_state.knowledge_base.load_with_cache()

if 'messages' not in st.session_state:
    st.session_state.messages = []

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
if st.session_state.knowledge_base.loaded_files:
    st.subheader("üìö –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
    for doc in sorted(st.session_state.knowledge_base.loaded_files):
        st.markdown(f"- {doc}")
else:
    st.warning("–í –ø–∞–ø–∫–µ docs –Ω–µ –Ω–∞–π–¥–µ–Ω–æ PDF-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞
if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    relevant_chunks = st.session_state.knowledge_base.find_most_relevant_chunks(prompt)
    
    if not relevant_chunks:
        response_text = "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö ‚ùå"
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        with st.chat_message("assistant"):
            st.markdown(response_text)
    else:
        context = "\n\n".join([f"–î–æ–∫—É–º–µ–Ω—Ç: {doc_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}\n{text}" 
                             for text, doc_name, page_num in relevant_chunks])
        
        full_prompt = f"""–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π —Å–æ–≤–µ—Ç–Ω–∏–∫.
        –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤ (–≤–Ω–∏–∑—É —É–∫–∞–∑–∞–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∏):

Question: {prompt}

Relevant materials:
{context}"""
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": full_prompt}],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        with st.spinner("–ò—â–µ–º –æ—Ç–≤–µ—Ç..."):
            start_time = datetime.now()
            
            try:
                response = requests.post(url, headers=headers, json=data)
                
                if response.status_code == 200:
                    response_data = response.json()
                    full_response = response_data['choices'][0]['message']['content']
                    
                    sources = "\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(
                        [f"- {doc_name}, —Å—Ç—Ä. {page_num}" for _, doc_name, page_num in relevant_chunks]
                    )
                    full_response += sources
                    
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response + " ‚úÖ")
                    
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    st.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {duration:.2f} —Å–µ–∫")
                else:
                    st.error(f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}")
            except Exception as e:
                st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")

# –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π"):
    st.session_state.messages = []
    st.rerun()

# –ö–Ω–æ–ø–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞
if st.button("–û–±–Ω–æ–≤–∏—Ç—å –∫—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
    st.session_state.knowledge_base = KnowledgeBase()
    st.session_state.knowledge_base.load_with_cache()
    st.rerun()
