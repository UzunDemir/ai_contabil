import streamlit as st
from PyPDF2 import PdfReader
from io import BytesIO
import requests
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import GPT2Tokenizer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
st.set_page_config(layout="wide", page_title="–ë—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π AI-—Å–æ–≤–µ—Ç–Ω–∏–∫")

class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.vectorizer = TfidfVectorizer()
        self.tfidf_matrix = None
        self.doc_texts = []
    
    def split_text(self, text, max_tokens=1500):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        for para in paragraphs:
            para = para.strip()
            if not para: continue
            tokens = tokenizer.tokenize(para)
            if len(tokenizer.tokenize(current_chunk + para)) > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = para
                else:
                    chunks.append(para)
                    current_chunk = ""
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        if current_chunk:
            chunks.append(current_chunk)
        return chunks

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
if 'kb' not in st.session_state:
    st.session_state.kb = KnowledgeBase()
if 'messages' not in st.session_state:
    st.session_state.messages = []

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("üßÆ –ë—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π AI-—Å–æ–≤–µ—Ç–Ω–∏–∫ –ú–æ–ª–¥–æ–≤—ã")
st.write("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Å –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∞–∫—Ç–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
uploaded_files = st.file_uploader(
    "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª—ã –∑–∞–∫–æ–Ω–æ–≤ (PDF)", 
    type="pdf", 
    accept_multiple_files=True
)

if uploaded_files and 'kb' in st.session_state:
    for uploaded_file in uploaded_files:
        if uploaded_file.name in [chunk.doc_name for chunk in st.session_state.kb.chunks]:
            continue
            
        try:
            with BytesIO(uploaded_file.getvalue()) as file:
                reader = PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                
                chunks = st.session_state.kb.split_text(text)
                for chunk in chunks:
                    st.session_state.kb.chunks.append(DocumentChunk(
                        text=chunk,
                        doc_name=uploaded_file.name,
                        page_num=0
                    ))
                    st.session_state.kb.doc_texts.append(chunk)
                
            st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {uploaded_file.name}")
            st.session_state.kb.tfidf_matrix = st.session_state.kb.vectorizer.fit_transform(
                st.session_state.kb.doc_texts
            )
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {uploaded_file.name}: {str(e)}")

# –ß–∞—Ç
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("–í–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ –±—É—Ö—É—á–µ—Ç—É/–Ω–∞–ª–æ–≥–∞–º..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    if not st.session_state.kb.chunks:
        response = "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã–µ –∞–∫—Ç—ã"
    else:
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        query_vec = st.session_state.kb.vectorizer.transform([prompt])
        similarities = cosine_similarity(query_vec, st.session_state.kb.tfidf_matrix)
        top_indices = np.argsort(similarities[0])[-3:][::-1]
        
        relevant_chunks = [
            (st.session_state.kb.chunks[i].text, 
             st.session_state.kb.chunks[i].doc_name, 
             st.session_state.kb.chunks[i].page_num)
            for i in top_indices if similarities[0][i] > 0.1
        ]
        
        if not relevant_chunks:
            response = "‚ùå –û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"
        else:
            context = "\n\n".join([f"üìÑ {doc} (—Å—Ç—Ä. {page}):\n{text}" 
                                 for text, doc, page in relevant_chunks])
            
            # –ò–º–∏—Ç–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ API)
            response = f"""–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞:

1. –û—Å–Ω–æ–≤–Ω–æ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ...
2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è...

üîç –ò—Å—Ç–æ—á–Ω–∏–∫–∏:
- {relevant_chunks[0][1]} (—Å—Ç—Ä. {relevant_chunks[0][2]})
- {relevant_chunks[1][1]} (—Å—Ç—Ä. {relevant_chunks[1][2]})"""
    
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)

if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
    st.session_state.messages = []
    st.rerun()
