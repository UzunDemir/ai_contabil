import os
import pickle
import hashlib
import streamlit as st
import requests
import numpy as np
import tempfile  # –î–æ–±–∞–≤–ª–µ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –∏–º–ø–æ—Ä—Ç
from PyPDF2 import PdfReader
from datetime import datetime
from transformers import GPT2Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import logging

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
logging.basicConfig(level=logging.INFO)
CACHE_DIR = "cache"
DOCS_DIR = "docs"
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(DOCS_DIR, exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# API –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –æ–±–ª–∞—Å—Ç—å –≤–∏–¥–∏–º–æ—Å—Ç–∏)
api_key = st.secrets.get("DEEPSEEK_API_KEY")
if not api_key:
    st.error("API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ Secrets.")
    st.stop()

url = "https://api.deepseek.com/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Streamlit
st.set_page_config(layout="wide", initial_sidebar_state="auto")
st.markdown("""
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    .css-1jc7ptx, .e1ewe7hr3, .viewerBadge_container__1QSob, 
    .styles_viewerBadge__1yB5_, .viewerBadge_link__1S137, 
    .viewerBadge_text__1JaDK, #MainMenu, footer, header { 
        display: none !important; 
    }
    .center {
        display: flex;
        justify-content: center;
        align-items: center;
        text-align: center;
        flex-direction: column;
        margin-top: 0vh;
    }
</style>
""", unsafe_allow_html=True)

st.sidebar.title("–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞")
st.sidebar.title("TEST-passer (AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Å—Ç–∞–º)")

class DocumentChunk:
    def __init__(self, text, doc_name, page_num):
        self.text = text
        self.doc_name = doc_name
        self.page_num = page_num

class KnowledgeBase:
    def __init__(self):
        self.chunks = []
        self.uploaded_files = []
        self.vectorizer = TfidfVectorizer(stop_words='english')
        self.tfidf_matrix = None
        self.doc_texts = []
    
    def split_text(self, text, max_tokens=2000):
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            tokens = tokenizer.tokenize(para)
            if len(tokenizer.tokenize(current_chunk + para)) > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = para
                else:
                    chunks.append(para)
                    current_chunk = ""
            else:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
        
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks
    
    def load_pdf(self, file_content, file_name):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name
            
            with open(tmp_file_path, 'rb') as file:
                reader = PdfReader(file)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        chunks = self.split_text(page_text)
                        for chunk in chunks:
                            self.chunks.append(DocumentChunk(
                                text=chunk,
                                doc_name=file_name,
                                page_num=page_num + 1
                            ))
                            self.doc_texts.append(chunk)
                
                if self.chunks:
                    self.uploaded_files.append(file_name)
                    # –û–±–Ω–æ–≤–ª—è–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
                    self.tfidf_matrix = self.vectorizer.fit_transform(self.doc_texts)
                    return True
                else:
                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ {file_name}")
                    return False
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PDF: {e}")
            return False
        finally:
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
    
    def find_most_relevant_chunks(self, query, top_k=3):
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é TF-IDF –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if not self.chunks:
            return []
            
        query_vec = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix)
        top_indices = np.argsort(similarities[0])[-top_k:][::-1]
        
        return [(self.chunks[i].text, self.chunks[i].doc_name, self.chunks[i].page_num) 
                for i in top_indices if similarities[0][i] > 0.1]
    
    def get_document_names(self):
        return self.uploaded_files
    
    def load_with_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏ docs —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞"""
        cache_file = os.path.join(CACHE_DIR, "knowledge_base.cache")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        pdf_files = [f for f in os.listdir(DOCS_DIR) if f.lower().endswith('.pdf')]
        if not pdf_files:
            return
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                cached_data = pickle.load(f)
                if cached_data['files'] == pdf_files:
                    self.chunks = cached_data['chunks']
                    self.uploaded_files = cached_data['files']
                    self.doc_texts = cached_data['doc_texts']
                    self.tfidf_matrix = self.vectorizer.fit_transform(self.doc_texts)
                    return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for file_name in pdf_files:
            file_path = os.path.join(DOCS_DIR, file_name)
            with open(file_path, 'rb') as file:
                self.load_pdf(file.read(), file_name)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if self.chunks:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'files': self.uploaded_files,
                    'chunks': self.chunks,
                    'doc_texts': self.doc_texts
                }, f)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
if 'knowledge_base' not in st.session_state:
    st.session_state.knowledge_base = KnowledgeBase()
    st.session_state.knowledge_base.load_with_cache()

if 'messages' not in st.session_state:
    st.session_state.messages = []

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
if st.session_state.knowledge_base.uploaded_files:  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å loaded_files –Ω–∞ uploaded_files
    st.subheader("üìö –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
    for doc in sorted(st.session_state.knowledge_base.uploaded_files):  # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ
        st.markdown(f"- {doc}")
else:
    st.warning("–í –ø–∞–ø–∫–µ docs –Ω–µ –Ω–∞–π–¥–µ–Ω–æ PDF-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞
if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    relevant_chunks = st.session_state.knowledge_base.find_most_relevant_chunks(prompt)
    
    if not relevant_chunks:
        response_text = "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö ‚ùå"
        st.session_state.messages.append({"role": "assistant", "content": response_text})
        with st.chat_message("assistant"):  # –û–ø–µ—á–∞—Ç–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ "assistant"
            st.markdown(response_text)
    else:
        context = "\n\n".join([f"–î–æ–∫—É–º–µ–Ω—Ç: {doc_name}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}\n{text}" 
                             for text, doc_name, page_num in relevant_chunks])
        
        full_prompt = f"""–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π —Å–æ–≤–µ—Ç–Ω–∏–∫.
        –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤ (–≤–Ω–∏–∑—É —É–∫–∞–∑–∞–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∏):

Question: {prompt}

Relevant materials:
{context}"""
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": full_prompt}],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        with st.spinner("–ò—â–µ–º –æ—Ç–≤–µ—Ç..."):
            start_time = datetime.now()
            
            try:
                response = requests.post(url, headers=headers, json=data)
                
                if response.status_code == 200:
                    response_data = response.json()
                    full_response = response_data['choices'][0]['message']['content']
                    
                    sources = "\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(
                        [f"- {doc_name}, —Å—Ç—Ä. {page_num}" for _, doc_name, page_num in relevant_chunks]
                    )
                    full_response += sources
                    
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response + " ‚úÖ")
                    
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    st.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {duration:.2f} —Å–µ–∫")
                else:
                    st.error(f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}")
            except Exception as e:
                st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")
                logging.error(f"API request error: {str(e)}")

# –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —á–∞—Ç–∞
if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π"):
    st.session_state.messages = []
    st.rerun()

# –ö–Ω–æ–ø–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞
if st.button("–û–±–Ω–æ–≤–∏—Ç—å –∫—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
    st.session_state.knowledge_base = KnowledgeBase()
    st.session_state.knowledge_base.load_with_cache()
    st.rerun()
